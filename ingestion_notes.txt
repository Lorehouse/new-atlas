How does texts.ingestion work?

For texts, the ingestion is done by calling texts.ingestion.ingest_texts(reset=True)
For dictionaries, ingestion is done by calling dictionaries.ingestion.ingest_dictionaries(reset=True)

ingest_texts:
reset=True filters for the node at the root of the tree (the node with field kind="nid") and deletes it, which also deletes the rest of the tree.

Things that happen:
1. if reset: Node.objects.filter(kind="nid").delete()
    this deletes the root node (nid is node id)
2. library = resolve_library()
        the __init__ uses LibraryDataResolver(LIBRARY_DATA_PATH).resolved to bind text_groups, works, versions
            LIBRARY_DATA_PATH represents the path from f"{settings.ATLAS_DATA_DIR}/texts"
                currently, this is BASE_DIR.parent / "test_data", i.e. "new-atlas/test_data", with "/texts" added on
            LibraryDataResolver takes the path passed to it and calls self.resolve_data_dir_path(path)
                self.resolve_data_dir_path walks(path) through directories and files. Each directory gets represented by dir_path.
                    Any directory without "metadata.json" gets skipped.
                    Otherwise, "metadata.json" gets loaded into metadata variable.
                    if "node_kind" == "textgroup", then self.text_groups gets the key:value pair <urn>: <contents of metadata.json>
                    elif "node_kind" == "work", then self.works gets the key:value pair <urn>: <contents of metadata.json>
                    and self.populate_versions(dirpath, metadata["versions"]) gets called
                        for "work" nodes, self.populate_versions takes the dirpath and the data in metadata.json["versions"]
                        It then loops through each json array in metadata.json["versions"].
                        It then grabs the value for the key "urn" and splits this string at ":" with a maximum of two splits from end of string.
                        and combines them into a list. It then grabs the second element in this list,
                        which could be something like "tlg0012.tlg002.perseus-grc2" or "urn:cts:greekLit:tlg0012.tlg002.perseus-eng3". This is the version_part.
                        It then for each array gets value for the key "format" for the file extension. If none is found, it defaults to ".txt".
                        version_path is bound as f"{dir_path}/{version_part}.{extension}".
                        Then, the attribute self.versions gets added to it the key:value pair
                            version["urn"]: {**version, "format": extension, "path": version_path}.
                            self.version contains a dictionary with a key: value pair for all the data specific to the version, in addition to a key: value pair for the extension and a pair for the version path.
                    self.resolve_data_dir_path has at this point added to the self.text_groups, self.works, and self.versions attributes.
                    self.resolve_data_dir_path then returns self.text_groups, self.works, and self.versions.
        At this point, resolve_library() has bound dicts to text_groups, works, and versions.
        It then returns Library(text_groups, works, versions), the __init__ of which binds these to attributes.
    now, library is a Library object with the attributes self.text_groups, self.works, and self.versions.
3. importer_class = CTSImporter this binds importer_class to the class itself
4. create empty dictionary: nodes
5. to_ingest = list(library.versions.values())  in essence, the continuous texts correspond to entries in a version dictionary, which however does not
        contain the texts themselves. The keys here are the urns. Notably, the dicts in versions.values() contain the urns for specific versions, from which one could extract the urn for the work.
6. create empty list: to_defer . This will be passed to CTSImporter __init__.
7. create lookup=None. THis will be passed to CTSImpoter __init__.
8. create control structure: with tqdm() as pbar
    tqdm() must be the constructor for an class that implements __getitem__ (to be subscriptable), __enter__ and __exit__ (for with keyword)
    this is manual control of tqdm().
    This usage won't give an estimate for how long the process will take. But the pbar does count how many iterations.
    tqdm().update(n=1) updates an internal counter according to whatever number is passed to it.
    create control structure: loop through version_data (i.e. the dict of data for a given version) for each version in to_ingest list.
        In each loop, CTSImporter.__init__ constructs a new importer. __init__ takes (library, version_data, nodes, lookup).
            __init__ extracts work urn from version_data. The purpose of CTSImporter is to make it easier to work with the CTS format data in version_data. In the future, it might be a good idea not to require that texts be ingested using CTSImporter (and other code assuming CTS format), and allow other Importer classes. Its supposed to be loaded with one version at a time (but with access to a whole library).
        deferred_nodes = CTSImporter.apply(). For managing the URN of the version and work, it uses the urn.URN class. Relevant here is that with URN self.absolute returns the full urn.
            Here, CTSImporter.apply() grabs the path for the urn of the version, and tries to use it to open a file. Then it reads the file line by line and passes each line to CTSImporter.generate_branch(line).
                For each line, generate_branch extracts node_urn and text_content using self.extract_urn_and_text_content(line). This method is able to extract from cex format. It pulls out the urn with e.g. line number, and returns it with the text itself.
                    Then self.extract_urn_and_text_content populates branch_data with self.destructure_urn(node_urn, text_content).
                        self.destructure_urn gets various data for the urn, and adds them to a dict data. It then returns a list with dicts for relevant pieces of data.
                self.generate_branch populates various attributes of CTSImporter instance, in particular the list self.nodes_to_create, which gets returned by self.apply(). self.nodes_to_create contains necessary data to ingest a version of a work.
            Then CTSImporter.apply() returns self.nodes_to_create, which gets bound to deferred_nodes.
        deferred_nodes is a list of all the nodes to be created for version of a work. It then gets added to the list to_defer.
        The number of deferred_nodes in this iteration (i.e. the number of lines of passages of text with a unique urn) gets added to the tqdm counter.
        lookup gets updated with lookup = importer.node_last_child_lookup attribute. I'm not entirely sure what the point of this is.
9.chunked_bulk_create(Node, to_defer). This function gets passed the Node class, and to_defer, which is a lost of all the nodes that need to be created for the different versions. Notably, the Node class uncludes all the core information for a given node, including where relevant text_content.
    chunked_bulk_create is in atlas.utils. It takes in an iterable and a model (here, Node). It gets the size of the iterable (unless total is passed as argument). It uses a lazy_iterable generator and islice to break iterable into subsets. It calls models.objects.bulk_create() to add each subset to database. Notably, Node.objects.bulk_create() gets this method from the standard model Manager for django models.




